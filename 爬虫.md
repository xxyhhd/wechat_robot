爬虫分类：　通用爬虫、聚焦爬虫

１、定位ＵＲＬ
２、

## urllib

### 实例一
```python
import urllib.request
import urllib.parse  # 解决参数配置问题

url = 'http://www.baidu.com/s?'
data = {
    'wd': '美女',
    'pass': '性别'
}

urllib.request.urlretrieve(url=url, filename='baidu.html')　　# 保存到本地

data1 = urllib.parse.urlencode(data)
url = url + data1

print(url)
```

### 实例二
```python
import urllib.request

url = 'http://www.baidu.com'
response = urllib.request.urlopen(url=url)
print(type(response))  # 获取类型, <class 'http.client.HTTPResponse'>
print(response.getcode())  # 　获取状态码, 200
print(response.geturl()) # 获取ＵＲＬ
print(response.getheaders())  # 获取头信息

# 编码：object-->字节　encode：对字典类型，　quote：　对单个字符串
# 解码：字节-->object  decode：对字典类型， unquote：　对单个字符串
print(response.read().decode('utf-8'))  # utf-8解码,获取页面源码，readline()读取一行，readlines(num)读取多行
```

### 实例三，修改头信息
```python
import urllib.request

url = 'http://www.baidu.com'
headers = {
    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36'
}

request = urllib.request.Request(url=url, headers=headers)  # request 相当于url, 可以加其他参数
respones = urllib.request.urlopen(request)
print(respones.getcode())
```

### 爬取豆瓣电影
```python
import urllib.request
import urllib.parse
import json


def get_tag():
    url = 'https://movie.douban.com/j/search_tags?type=movie&source='
    respones = urllib.request.urlopen(url=url)
    content = respones.read().decode('utf-8')
    my_json = json.loads(content)
    info = '  '.join(my_json['tags'])
    print('可选电影种类：', info)
    return my_json['tags']


def create_request(a):
    tag = str(input('请输入查询的电影类型：'))
    cate = urllib.parse.quote(tag)
    num = int(input('请输入你需要查询的数量：'))
    page = num//20
    remainder = num % 20
    print('{0:^5}\t{1:{3}^３0}\t{2:^８}'.format('排名', '电影名称', '评分', chr(12288)))
    for x in range(page):
        data = {
            'page_start': x*20,
        }
        data = data = urllib.parse.urlencode(data)
        url = 'https://movie.douban.com/j/search_subjects?type=movie&page_limit=20&sort=rank&' + \
            'tag=' + cate + '&' + data
        respones = urllib.request.urlopen(url=url)
        content = respones.read().decode('utf-8')
        my_json = json.loads(content)
        count = x*20
        for move in my_json['subjects']:
            count += 1
            print('{0:^5}\t{1:{3}^３0}\t{2:^10}'.format(
                count, move['title'], move['rate'], chr(12288)))
    url = 'https://movie.douban.com/j/search_subjects?type=movie&page_limit=20&sort=rank&' + \
        'tag=' + cate + '&' + data
    respones = urllib.request.urlopen(url=url)
    content = respones.read().decode('utf-8')
    my_json = json.loads(content)
    count = page*20
    for move in my_json['subjects'][0:remainder]:
        count += 1
        print('{0:^5}\t{1:{3}^３0}\t{2:^10}'.format(
            count, move['title'], move['rate'], chr(12288)))


def main():
    # get_tag()
    create_request(get_tag())


if __name__ == '__main__':
    main()
```

### habdler
HTTPHandler, 普通handler对象，可以配置一般请求  
HTTPCookieProcessor,　配置cookie  
HTTPProxyHandler, 配置代理  

```python
import urllib.request
import urllib.parse

url = 'http://www.baidu.com'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36',
}

req = urllib.request.Request(url=url, headers=headers)

# 构造更高级的请求对象
handler = urllib.request.HTTPHandler()
opener = urllib.request.build_opener(handler)
response = opener.open(req)

print(response.read().decode('utf-8'))
```

### 人人网，登录
```python
import urllib.request
import urllib.parse
import http.cookiejar
import json


cookie = http.cookiejar.CookieJar()
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)


post_url = 'http://www.renren.com/ajaxLogin/login?1=1&uniqueTimestamp=2019651354277'
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36',
}
form_data = {
    'email': '15620569943',
    'icode': '', 
    'origURL': 'http://www.renren.com/home',
    'domain': 'renren.com',
    'key_id': '1',
    'captcha_type': 'web_login',
    'password': '859a12c8d87dd25ee379ae206146390020d298612535bbb59cf29bacd5e2402e',
    'rkey': 'd0cf42c2d3d337f9e5d14083f2d52cb2',
    'f':'', 
}
# post做数据编码
data = urllib.parse.urlencode(form_data).encode('utf-8')
req = urllib.request.Request(data=data, url=post_url, headers=headers)
response = opener.open(req)

print(response.getcode())

my_url = 'http://www.renren.com/551403626/profile'
my_req = urllib.request.Request(url=my_url)
my_response = opener.open(my_req)
print(my_response.read().decode('utf-8'))
```

### Xpath语法

xpath 使用：   
+ 安装lxml: pip install lxml   
+ 导入：from lxml import etree   
+ 使用：html_tree = etree.pares('html文件')　or html_tree = etree.HTML(response.read().decode('utf-8'))   
+ xpant常用规则
    +   nodename	    选择这个节点名的所有子节点
    +       /	            从当前节点选择直接子节点
    +       //	        从当前节点选取子孙节点
    +       .	            选择当前节点
    +       …	            选取当前节点的父节点
    +       @	            选取属性
+ html = etree.parse('./test.html',etree.HTMLParser())  
    + result = html.xpath('//ul//a/text()') 获取所有ul节点下的所有a节点的文本部分
    + result = html.xpath('//li[@class="item-3"]') 获取所有class属性值为item-3的li节点,返回列表形式
    + result = html.xpath('//li[contains(@class,"item-0") and @name="one"]/a/text()') 多属性查询用contains



### Beautiful Soup 4
Beautiful Soup支持的解析器   
1. Python标准库	
    + 使用：BeautifulSoup(markup, “html.parser”) 
    + 优点：Python的内置标准库、执行速度适中、文档容错能力强 
    + 缺点：Python 2.7.3及Python 3.2.2之前的版本文档容错能力差
2. xml HTML解析器	
    + 使用：BeautifulSoup(markup, “lxml”) 
    + 优点：速度快、文档容错能力强 
    + 缺点：需要安装C语言库
3. lxml XML解析器	
    + 使用：BeautifulSoup(markup, “xml”) 
    + 优点：速度快、唯一支持XML的解析器 
    + 缺点：需要安装C语言库
4. html5lib	
    + 使用：BeautifulSoup(markup, “html5lib”) 
    + 优点：最好的容错性、以浏览器的方式解析文档、生成HTML5格式的文档 
    + 缺点：速度慢、不依赖外部扩展
#### 使用
+ 导入：from bs4 import BeautifulSoup  
+ 创建soup对象：soup = BeautifulSoup(open('html文件'), 'lxml') or soup = BeautifulSoup(抓取的页面, 'lxml')    
+ 优雅的打印HTML: print(soup.prettify())  
1. 节点选择器， 直接调用节点的名称就可以选择节点元素
    + 嵌套选择：每一个返回结果都是bs4.element.Tag类型，它同样可以继续调用节点进行下一步的选择。比如，我们获取了head节点元素，
    我们可以继续调用head来选取其内部的head节点元素
    + 获取某个元素：print(soup.p) 又多个p元素时之后选择第一个
    + 获取元素的类型：print(type(soup.title))  <class 'bs4.element.Tag'>
    + 获取元素的文本部分：print(soup.title.string) or soup.get_text() 推荐使用get
    + 获取元素的name: print(soup.title.name)
    + 获取元素的全部属性：print(soup.p.attrs)  --{'class': ['title'], 'name': 'dromouse'}
    + print(soup.p.attrs['name'])  --dromouse  
    <!-- 这里需要注意的是，有的返回结果是字符串，有的返回结果是字符串组成的列表。比如，name属性的值是唯一的，  
    返回的结果就是单个字符串。而对于class，一个节点元素可能有多个class，所以返回的是列表。在实际处理过程中，我们要注意判断类型 -->
2. 关联选择：在做选择的时候，有时候不能做到一步就选到想要的节点元素，需要先选中某一个节点元素，然后以它为基准再选择它的子节点、父节点、兄弟节点等
    + 获取p节点的所有直接子节点，返回生成器：soup.p.children
    + 获取p节点的所有直接子节点，返回列表：soup.p.contents
    + 获取p节点的所有所有子孙节点，返回生成器：soup.p.descendants
    + 获取a节点的直接父节点：soup.a.parent
    + 获取a节点的所有祖辈节点，返回生成器：soup.a.parents
    + next_sibling和previous_sibling分别获取节点的下一个和上一个兄弟元素
    + next_siblings和previous_siblings则分别返回所有前面和后面的兄弟节点的生成器
    <!-- 如果返回结果是单个节点，那么可以直接调用string、attrs等属性获得其文本和属性；如果返回结果是多个节点的生成器，则可以转为列表后取出某个元素，然后再调用string、attrs等属性获取其对应节点的文本和属性 -->
3. 方法选择器
    + find_all(name , attrs , recursive , text , **kwargs)
        + name节点类型：soup.find_all(name='ul')  ，查询所有ul节点，返回结果是列表类型
        + attrs属性查询: soup.find_all(attrs={'id': 'list-1'})，返回列表类型
        + 对于一些常用的属性，比如id和class等，我们可以不用attrs来传递
            + soup.find_all(id='list-1'))，返回列表类型
            + soup.find_all(class_='element')，返回列表类型
        + text匹配节点的文本，传入的形式可以是字符串，可以是正则表达式对象: soup.find_all(text=re.compile('link'))
    + find(), 返回匹配的第一个元素
        + soup.find(name='ul'
        + soup.find(class_='list')
    + find_parents()和find_parent()：前者返回所有祖先节点，后者返回直接父节点。 
    + find_next_siblings()和find_next_sibling()：前者返回后面所有的兄弟节点，后者返回后面第一个兄弟节点。 
    + find_previous_siblings()和find_previous_sibling()：前者返回前面所有的兄弟节点，后者返回前面第一个兄弟节点。 
    + find_all_next()和find_next()：前者返回节点后所有符合条件的节点，后者返回第一个符合条件的节点。 
    + find_all_previous()和find_previous()：前者返回节点后所有符合条件的节点，后者返回第一个符合条件的节点
4. CSS选择器，只需要调用select()方法，传入相应的CSS选择器即可
    + soup.select('ul li')
    + 部分CSS选择器
        + .class   
        .intro   
        选择class="intro"的所有元素。	
        + #id   
        #firstname   
        选择id="firstname"的所有元素。	       
        + element  
         p  
        选择所有p元素。	
        + element,element   
        div,p  
         选择所有div元素和所有p元素。
        + element element   
        div p   
        选择div元素内部的所有p元素。	
        + element>element  
         div>p 选择父元素为   
         div元素的所有p元素。
        + element+element  
         div+p   
         选择紧接在div元素之后的所有p元素。
        + attribute	        
        target	        
        选择带有target属性所有元素。
        + attribute=value	    
        target=_blank	    
        选择target="_blank"的所有元素。
        + attribute~=value	  
        title~=flower    
        选择title属性包含单词"flower"的所有元素。	
        + attribute|=value        
        lang|=en	        
        选择lang属性值以"en"开头的所有元素。

### bgk 和　gb123区别：都支持中文，但是gbk支持繁体中文

### selenium:动态加载JS